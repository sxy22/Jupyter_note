{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dbe4e9528eda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 继承Module类来构造模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 声明带有模型参数的层，这里声明了两个全连接层\n",
    "    def __init__(self, **kwargs):\n",
    "        # 调用MLP父类Module的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\n",
    "        # 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数params\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Linear(5, 3) # 隐藏层\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(3, 1)  # 输出层\n",
    "\n",
    "\n",
    "    # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\n",
    "    def forward(self, x):\n",
    "        a = self.act(self.hidden(x))\n",
    "        return self.output(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当模型的前向计算为简单串联各个层的计算时，Sequential类可以通过更加简单的方式定义模型。  \n",
    "这正是Sequential类的目的：它可以接收一个子模块的有序字典（OrderedDict）或者一系列子模块作为参数来逐一添加Module的实例，  \n",
    "而模型的前向计算就是将这些实例按添加的顺序逐一计算。  \n",
    "\n",
    "下面我们实现一个与Sequential类有相同功能的MySequential类。这或许可以帮助读者更加清晰地理解Sequential类的工作机制。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    from collections import OrderedDict\n",
    "    def __init__(self, *args):\n",
    "        super(MySequential, self).__init__()\n",
    "        if len(args) == 1 and isinstance(args[0], OrderedDict): # 如果传入的是一个OrderedDict\n",
    "            for key, module in args[0].items():\n",
    "                self.add_module(key, module)  # add_module方法会将module添加进self._modules(一个OrderedDict)\n",
    "        else:  # 传入的是一些Module\n",
    "            for idx, module in enumerate(args):\n",
    "                self.add_module(str(idx), module)\n",
    "    def forward(self, input):\n",
    "        # self._modules返回一个 OrderedDict，保证会按照成员添加时的顺序遍历成员\n",
    "        for module in self._modules.values():\n",
    "            input = module(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MySequential(\n",
    "        nn.Linear(784, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10), \n",
    "        )\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 可以通过继承Module类来构造模型。  \n",
    "+ Sequential、ModuleList、ModuleDict类都继承自Module类。  \n",
    "+ 与Sequential不同，ModuleList和ModuleDict并没有定义一个完整的网络，它们只是将不同的模块存放在一起，需要自己定义forward函数。  \n",
    "+ 虽然Sequential等类可以使模型构造更加简单，但直接继承Module类可以极大地拓展模型构造的灵活性。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数及初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1))  # pytorch已进行默认初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net1 = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[ 0.0017, -0.0090, -0.0178,  0.0006],\n",
      "        [-0.0129,  0.0072, -0.0127,  0.0072],\n",
      "        [ 0.0021, -0.0057, -0.0134,  0.0087]])\n",
      "2.weight tensor([[ 0.0175, -0.0124, -0.0074]])\n"
     ]
    }
   ],
   "source": [
    "# 对weight进行正态初始化\n",
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init.normal_(param, mean=0, std=0.01)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对bias进行0初始化\n",
    "for name, param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        init.constant_(param, val=0)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用线性回归模型真实权重 w=[2,−3.4]⊤ 和偏差 b=4.2以及一个随机噪声项 ϵϵ 来生成标签   \n",
    "y=Xw+b+ϵ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成1000个数据\n",
    "torch.manual_seed(10)\n",
    "num_inputs = 2\n",
    "num_examples = 100\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = torch.randn(num_examples, num_inputs, dtype=torch.float32)\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.1, size=labels.size()), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每次返回batch_size（批量⼤小）个随机样本的特征和标签\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)  # 样本的读取顺序是随机的\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # 最后一次可能不足一个batch\n",
    "        yield  features.index_select(0, j), labels.index_select(0, j)\n",
    "\n",
    "#定义模型\n",
    "def linreg(X, w, b):  \n",
    "    return torch.mm(X, w) + b\n",
    "\n",
    "# 损失函数\n",
    "def squared_loss(y_hat, y):  \n",
    "    # print(y.view(y_hat.size()))\n",
    "    return (y_hat - y.view(y_hat.size())) ** 2 / 2\n",
    "\n",
    "def sgd(params, lr, batch_size):  # 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 9.067955\n",
      "epoch 2, loss 4.756135\n",
      "epoch 3, loss 2.508253\n",
      "epoch 4, loss 1.328605\n",
      "epoch 5, loss 0.707993\n",
      "epoch 6, loss 0.379959\n",
      "epoch 7, loss 0.206142\n",
      "epoch 8, loss 0.113420\n",
      "epoch 9, loss 0.063855\n",
      "epoch 10, loss 0.037072\n"
     ]
    }
   ],
   "source": [
    "# 初始化参数\n",
    "w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32, requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "lr = 0.03\n",
    "num_epochs = 10\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):  # 训练模型一共需要num_epochs个迭代周期\n",
    "    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X\n",
    "    # 和y分别是小批量样本的特征和标签\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y).sum()  # l是有关小批量X和y的损失\n",
    "        l.backward()  # 小批量的损失对模型参数求梯度\n",
    "        sgd([w, b], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数\n",
    "\n",
    "        # 不要忘了梯度清零\n",
    "        w.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -3.4] \n",
      " tensor([[ 1.8336],\n",
      "        [-3.2457]], requires_grad=True)\n",
      "4.2 \n",
      " tensor([4.0469], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(true_w, '\\n', w)\n",
    "print(true_b, '\\n', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成1000个数据\n",
    "torch.manual_seed(10)\n",
    "num_inputs = 2\n",
    "num_examples = 100\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = torch.randn(num_examples, num_inputs, dtype=torch.float32)\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.1, size=labels.size()), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "batch_size = 10\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = Data.TensorDataset(features, labels)\n",
    "# 随机读取小批量\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "#            batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "#            pin_memory=False, drop_last=False, timeout=0,\n",
    "#            worker_init_fn=None, *, prefetch_factor=2,\n",
    "#            persistent_workers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义一个nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(n_feature, 1)\n",
    "    # forward 定义前向传播\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "\n",
    "net = LinearNet(num_inputs)\n",
    "print(net) # 使用print可以打印出网络的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事实上我们还可以用nn.Sequential来更加方便地搭建网络，Sequential是一个有序的容器，\n",
    "# 网络层将按照在传入Sequential的顺序依次被添加到计算图中。\n",
    "\n",
    "# 写法一\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(num_inputs, 1)\n",
    "    # 此处还可以传入其他层\n",
    "    )\n",
    "\n",
    "# 写法二\n",
    "net = nn.Sequential()\n",
    "net.add_module('linear', nn.Linear(num_inputs, 1))\n",
    "# net.add_module ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4271, -0.3423]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.5118], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.7674]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0857], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 可以通过net.parameters()来查看模型所有的可学习参数，此函数将返回一个生成器。\n",
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用net前，我们需要初始化模型参数，如线性回归模型中的权重和偏差。PyTorch在init模块中提供了  \n",
    "多种参数初始化方法。这里的init是initializer的缩写形式。我们通过init.normal_将权重参数每个元素初  \n",
    "始化为随机采样于均值为0、标准差为0.01的正态分布。偏差会初始化为零。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0942], requires_grad=True)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import init\n",
    "\n",
    "init.normal_(net.linear.weight, mean=0, std=0.1)\n",
    "init.constant_(net.linear.bias, val=0)  # 也可以直接修改bias的data: net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，我们也无须自己实现小批量随机梯度下降算法。torch.optim模块提供了很多常用的优  \n",
    "化算法比如SGD、Adam和RMSProp等。下面我们创建一个用于优化net所有参数的优化器实例，  \n",
    "并指定学习率为0.03的小批量随机梯度下降（SGD）为优化算法。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.03\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.03)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 7.204216\n",
      "epoch 2, loss: 3.043922\n",
      "epoch 3, loss: 0.444130\n",
      "epoch 4, loss: 0.328976\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        output = net(X)\n",
    "        # print(output.shape)\n",
    "        l = loss(output, y.view(output.size()))\n",
    "        optimizer.zero_grad() # 梯度清零，等价于net.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d, loss: %f' % (epoch, l.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -3.4] Parameter containing:\n",
      "tensor([[ 1.7104, -3.1337]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(true_w, net.linear.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2 Parameter containing:\n",
      "tensor([3.9224], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(true_b, net.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.FashionMNIST(root='F:/Jupyter_note/DL_pytorch/FashionMNIST', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='F:/Jupyter_note/DL_pytorch/FashionMNIST', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(mnist_train), len(mnist_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion-MNIST中一共包括了10个类别，分别为t-shirt（T恤）、trouser（裤子）、pullover（套衫）、  \n",
    "dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle boot（短靴）。  \n",
    "以下函数可以将数值标签转成相应的文本标签。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "if sys.platform.startswith('win'):\n",
    "    num_workers = 0  # 0表示不用额外的进程来加速读取数据\n",
    "else:\n",
    "    num_workers = 4\n",
    "train_iter = Data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_iter = Data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.00 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for X, y in train_iter:\n",
    "    continue\n",
    "print('%.2f sec' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在3.4节（softmax回归）中提到，softmax回归的输出层是一个全连接层，所以我们用一个线性模块就可以了。 \n",
    "因为前面我们数据返回的每个batch样本x的形状为(batch_size, 1, 28, 28),   \n",
    "所以我们要先用view()将x的形状转换成(batch_size, 784)才送入全连接层。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "    def forward(self, x): # x shape: (batch, 1, 28, 28)\n",
    "        y = self.linear(x.view(x.shape[0], -1))\n",
    "        return y\n",
    "\n",
    "net = LinearNet(num_inputs, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "init.normal_(net.linear.weight, mean=0, std=0.1)\n",
    "init.constant_(net.linear.bias, val=0) \n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0032, train acc 0.730, test acc 0.777\n",
      "epoch 2, loss 0.0023, train acc 0.805, test acc 0.804\n",
      "epoch 3, loss 0.0021, train acc 0.821, test acc 0.812\n",
      "epoch 4, loss 0.0020, train acc 0.827, test acc 0.819\n",
      "epoch 5, loss 0.0019, train acc 0.833, test acc 0.824\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y).sum()\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度并更新\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        train_l_sum += l.item()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        n += y.shape[0]\n",
    "    test_acc = evaluate_accuracy(test_iter, net)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "          % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import init\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def get_data_iter():\n",
    "    # 读取数据\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='F:/Jupyter_note/DL_pytorch/FashionMNIST',\n",
    "                                                    train=True, download=True, transform=transforms.ToTensor())\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root='F:/Jupyter_note/DL_pytorch/FashionMNIST',\n",
    "                                                   train=False, download=True, transform=transforms.ToTensor())\n",
    "    # 生成iter\n",
    "    batch_size = 256\n",
    "    num_workers = 0  # 0表示不用额外的进程来加速读取数据\n",
    "    train_iter = Data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_iter = Data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear_1 = nn.Linear(num_inputs, num_hiddens)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(num_hiddens, num_outputs)\n",
    "        \n",
    "    def forward(self, x): # x shape: (batch, 1, 28, 28)\n",
    "        y = x.view(x.shape[0], -1)\n",
    "        y = self.linear_1(y)\n",
    "        y = self.ReLU(y)\n",
    "        y = self.linear_2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "net = Net(num_inputs, num_outputs, num_hiddens)\n",
    "for params in net.parameters():\n",
    "    init.normal_(params, mean=0, std=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = get_data_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0034, train acc 0.732, test acc 0.808\n",
      "epoch 2, loss 0.0018, train acc 0.830, test acc 0.818\n",
      "epoch 3, loss 0.0016, train acc 0.847, test acc 0.846\n",
      "epoch 4, loss 0.0015, train acc 0.862, test acc 0.829\n",
      "epoch 5, loss 0.0014, train acc 0.869, test acc 0.828\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y).sum()\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度并更新\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        # 计算预测acc\n",
    "        train_l_sum += l.item()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        n += y.shape[0]\n",
    "    test_acc = evaluate_accuracy(test_iter, net)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "          % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    net.eval()\n",
    "    for X, y in data_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    net.train()\n",
    "    return acc_sum / n\n",
    "\n",
    "def get_data_iter():\n",
    "    # 读取数据\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='F:/Jupyter_note/DL_pytorch/FashionMNIST',\n",
    "                                                    train=True, download=True, transform=transforms.ToTensor())\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root='F:/Jupyter_note/DL_pytorch/FashionMNIST',\n",
    "                                                   train=False, download=True, transform=transforms.ToTensor())\n",
    "    # 生成iter\n",
    "    batch_size = 256\n",
    "    num_workers = 0  # 0表示不用额外的进程来加速读取数据\n",
    "    train_iter = Data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_iter = Data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):# img (batch, 28, 28)\n",
    "        feature = self.conv(img)# feature (batch, 16, 4, 4)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): Sigmoid()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=120, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv.0.weight Parameter containing:\n",
      "tensor([[[[-0.0447,  0.1498,  0.1664, -0.1300, -0.1933],\n",
      "          [-0.0044, -0.0787,  0.0704, -0.1642, -0.0604],\n",
      "          [-0.0275,  0.0372,  0.0364, -0.1809,  0.0630],\n",
      "          [-0.1100,  0.0910, -0.0143, -0.1816,  0.1740],\n",
      "          [ 0.1726, -0.0012,  0.0366, -0.1901,  0.0428]]],\n",
      "\n",
      "\n",
      "        [[[-0.0514, -0.0151, -0.0135,  0.0042, -0.1135],\n",
      "          [ 0.1302, -0.0471,  0.1057, -0.1515,  0.1249],\n",
      "          [-0.0178,  0.0595,  0.0802,  0.1590, -0.1888],\n",
      "          [-0.1518, -0.1957, -0.1996, -0.1830, -0.1277],\n",
      "          [-0.1735,  0.0279,  0.0448, -0.0790, -0.0608]]],\n",
      "\n",
      "\n",
      "        [[[-0.1874,  0.0503,  0.0393,  0.0466,  0.0062],\n",
      "          [ 0.0718,  0.0105,  0.1346,  0.0446, -0.0668],\n",
      "          [ 0.1350,  0.1670,  0.1149,  0.0852, -0.0159],\n",
      "          [ 0.1259, -0.0072, -0.0493,  0.0628, -0.0880],\n",
      "          [ 0.1495, -0.0791, -0.1339,  0.1067, -0.0509]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1717,  0.1382,  0.0347,  0.1180,  0.1857],\n",
      "          [ 0.0899,  0.0757,  0.0455,  0.1149, -0.1323],\n",
      "          [-0.0667, -0.0753,  0.1420,  0.1121,  0.1838],\n",
      "          [-0.0764,  0.1310, -0.1142, -0.1349,  0.0572],\n",
      "          [-0.0131,  0.0865,  0.1453, -0.1334, -0.1956]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1871,  0.0811,  0.0395,  0.1490, -0.1819],\n",
      "          [ 0.0541,  0.1864,  0.1453,  0.1685, -0.1244],\n",
      "          [ 0.0072, -0.0849, -0.1900,  0.0492, -0.1776],\n",
      "          [-0.1544, -0.0708,  0.1414,  0.1924, -0.0834],\n",
      "          [ 0.0302, -0.1555,  0.0856, -0.0673,  0.0409]]],\n",
      "\n",
      "\n",
      "        [[[-0.0489, -0.0215,  0.0788,  0.1556,  0.0142],\n",
      "          [ 0.0828, -0.1117, -0.1775, -0.0803,  0.0540],\n",
      "          [ 0.0865, -0.0230, -0.0333,  0.0616,  0.1600],\n",
      "          [-0.0345, -0.0231, -0.1664,  0.0622, -0.0987],\n",
      "          [ 0.1125,  0.1671, -0.0656,  0.0842, -0.0151]]]], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv.0.weight Parameter containing:\n",
      "tensor([[[[ 0.1408, -0.1915, -0.0606,  0.1904,  0.1267],\n",
      "          [-0.1221,  0.0621, -0.1017, -0.1557,  0.0313],\n",
      "          [ 0.1557,  0.1507,  0.0728,  0.0564, -0.0363],\n",
      "          [ 0.1698, -0.0343,  0.0551,  0.1881,  0.1608],\n",
      "          [ 0.1054, -0.0694,  0.1368, -0.1188,  0.0069]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0129,  0.1824,  0.1115,  0.1559, -0.0152],\n",
      "          [ 0.0184, -0.0912, -0.1593, -0.0969, -0.0770],\n",
      "          [-0.1178,  0.1054,  0.0763, -0.1982,  0.1377],\n",
      "          [-0.1782,  0.1849,  0.1206, -0.1703,  0.0546],\n",
      "          [ 0.0590, -0.0479,  0.0069, -0.0824, -0.1583]]],\n",
      "\n",
      "\n",
      "        [[[-0.1276,  0.1847,  0.1320, -0.1566, -0.1842],\n",
      "          [-0.1012,  0.0070, -0.0594, -0.0676, -0.1713],\n",
      "          [-0.1644,  0.1918, -0.1226, -0.0124, -0.1112],\n",
      "          [ 0.0790, -0.1671,  0.1654,  0.1232, -0.0453],\n",
      "          [-0.1147,  0.0073, -0.1968, -0.0542,  0.0527]]],\n",
      "\n",
      "\n",
      "        [[[-0.0663,  0.0003,  0.1822, -0.0942, -0.0629],\n",
      "          [ 0.0748, -0.1549, -0.0485, -0.1062,  0.1185],\n",
      "          [-0.1022, -0.0563,  0.0194,  0.0846, -0.0276],\n",
      "          [ 0.1470,  0.1781, -0.1019,  0.0060, -0.0815],\n",
      "          [-0.0717,  0.0900, -0.0900, -0.1441,  0.0428]]],\n",
      "\n",
      "\n",
      "        [[[-0.1493, -0.1084,  0.0880, -0.0406, -0.0719],\n",
      "          [ 0.0465,  0.1211, -0.0043, -0.0884, -0.1999],\n",
      "          [ 0.1366, -0.0146, -0.0908, -0.1927,  0.1593],\n",
      "          [ 0.0573, -0.1657, -0.1935,  0.0989, -0.1560],\n",
      "          [-0.0783, -0.1176,  0.0402, -0.0192, -0.0919]]],\n",
      "\n",
      "\n",
      "        [[[-0.1939,  0.0599, -0.0731, -0.0570, -0.1906],\n",
      "          [ 0.0895, -0.1014, -0.0441,  0.0993,  0.0057],\n",
      "          [-0.1449,  0.1613,  0.1563, -0.1065,  0.1444],\n",
      "          [ 0.1742, -0.0335,  0.1474, -0.1238,  0.0541],\n",
      "          [ 0.0835, -0.0486,  0.1802, -0.1314, -0.0746]]]], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = get_data_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, test acc 0.746, time 5\n",
      "epoch 2, test acc 0.812, time 11\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in train_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y).sum()\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度并更新\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    test_acc = evaluate_accuracy(test_iter, net)\n",
    "    print('epoch %d, test acc %.3f, time %d'\n",
    "          % (epoch + 1, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "167px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
